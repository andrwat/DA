{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>any</th>\n",
       "      <th>are</th>\n",
       "      <th>as</th>\n",
       "      <th>at</th>\n",
       "      <th>be</th>\n",
       "      <th>...</th>\n",
       "      <th>what</th>\n",
       "      <th>when</th>\n",
       "      <th>which</th>\n",
       "      <th>who</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>would</th>\n",
       "      <th>your</th>\n",
       "      <th>BookID</th>\n",
       "      <th>Author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Austen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    a  all  also  an  and  any  are  as  at  be   ...    what  when  which  \\\n",
       "0  46   12     0   3   66    9    4  16  13  13   ...       7     5      6   \n",
       "1  35   10     0   7   44    4    3  18  16   9   ...       5     7      7   \n",
       "\n",
       "   who  will  with  would  your  BookID  Author  \n",
       "0    8     4     9      1     0       1  Austen  \n",
       "1    3     5    14      8     0       1  Austen  \n",
       "\n",
       "[2 rows x 71 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import seaborn as sns\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "authorship = read_csv('../data/authorship.csv')\n",
    "authorship.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Austen', 'London', 'Shakespeare', 'Milton']\n"
     ]
    }
   ],
   "source": [
    "authors = list(set(authorship.Author.values))\n",
    "print authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "Name: Author_num, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(authors)\n",
    "authorship['Author_num'] = le.transform(authorship['Author'])\n",
    "print authorship['Author_num'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a random variable (random forests work best with a random variable)\n",
    "authorship['random'] = [random.random() for i in range(841)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'all', 'also', 'an', 'and', 'any', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', 'do', 'down', 'even', 'every', 'for', 'from', 'had', 'has', 'have', 'her', 'his', 'if', 'in', 'into', 'is', 'it', 'its', 'may', 'more', 'must', 'my', 'no', 'not', 'now', 'of', 'on', 'one', 'only', 'or', 'our', 'should', 'so', 'some', 'such', 'than', 'that', 'the', 'their', 'then', 'there', 'things', 'this', 'to', 'up', 'upon', 'was', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'would', 'your']\n"
     ]
    }
   ],
   "source": [
    "#What are some of the stop words we're looking at?\n",
    "features = list(authorship.columns)\n",
    "features.remove('Author')\n",
    "features.remove('Author_num')\n",
    "features.remove('BookID')\n",
    "print features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============no of estimators =  20  test_size =  0.1\n",
      "0.976244854045\n",
      "[[168   4   0   1]\n",
      " [  2 130   0   1]\n",
      " [  0   0  23   0]\n",
      " [  0   0   2  90]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  20  test_size =  0.2\n",
      "0.975088241665\n",
      "[[168   0   0   1]\n",
      " [  1 133   1   1]\n",
      " [  0   0  24   0]\n",
      " [  1   1   0  90]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  20  test_size =  0.3\n",
      "0.981040835538\n",
      "[[168   4   0   0]\n",
      " [  2 130   0   1]\n",
      " [  0   0  25   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  20  test_size =  0.4\n",
      "0.973863780181\n",
      "[[170   2   0   1]\n",
      " [  0 131   1   2]\n",
      " [  0   0  24   0]\n",
      " [  0   1   0  89]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  25  test_size =  0.1\n",
      "0.977465109006\n",
      "[[169   0   0   1]\n",
      " [  1 133   3   1]\n",
      " [  0   0  22   0]\n",
      " [  0   1   0  90]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  25  test_size =  0.2\n",
      "0.969101632452\n",
      "[[168   2   0   0]\n",
      " [  2 132   1   1]\n",
      " [  0   0  24   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  25  test_size =  0.3\n",
      "0.977473522053\n",
      "[[169   2   0   0]\n",
      " [  1 132   1   1]\n",
      " [  0   0  24   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  25  test_size =  0.4\n",
      "0.969186670804\n",
      "[[170   2   0   0]\n",
      " [  0 132   2   1]\n",
      " [  0   0  23   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  30  test_size =  0.1\n",
      "0.982222868635\n",
      "[[169   2   0   1]\n",
      " [  1 132   2   1]\n",
      " [  0   0  23   0]\n",
      " [  0   0   0  90]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  30  test_size =  0.2\n",
      "0.97865555515\n",
      "[[169   1   0   1]\n",
      " [  1 132   1   1]\n",
      " [  0   0  23   0]\n",
      " [  0   1   1  90]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  30  test_size =  0.3\n",
      "0.975092448189\n",
      "[[168   0   0   0]\n",
      " [  2 134   0   1]\n",
      " [  0   0  25   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  30  test_size =  0.4\n",
      "0.984591141352\n",
      "[[169   2   0   0]\n",
      " [  1 131   1   2]\n",
      " [  0   0  24   0]\n",
      " [  0   1   0  90]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  35  test_size =  0.1\n",
      "0.979816374053\n",
      "[[169   1   0   0]\n",
      " [  1 133   0   1]\n",
      " [  0   0  25   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  35  test_size =  0.2\n",
      "0.97980777943\n",
      "[[168   0   0   0]\n",
      " [  2 134   1   1]\n",
      " [  0   0  24   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  35  test_size =  0.3\n",
      "0.981019621344\n",
      "[[168   2   0   0]\n",
      " [  2 131   0   1]\n",
      " [  0   0  25   0]\n",
      " [  0   1   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  35  test_size =  0.4\n",
      "0.978676950921\n",
      "[[169   1   0   0]\n",
      " [  1 133   1   1]\n",
      " [  0   0  24   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  40  test_size =  0.1\n",
      "0.979846001294\n",
      "[[169   2   0   0]\n",
      " [  1 132   1   1]\n",
      " [  0   0  24   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  40  test_size =  0.2\n",
      "0.982222868635\n",
      "[[169   0   0   0]\n",
      " [  1 134   3   1]\n",
      " [  0   0  21   0]\n",
      " [  0   0   1  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  40  test_size =  0.3\n",
      "0.979854595917\n",
      "[[169   2   0   0]\n",
      " [  1 132   2   1]\n",
      " [  0   0  23   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  40  test_size =  0.4\n",
      "0.981023827867\n",
      "[[169   1   0   0]\n",
      " [  1 133   1   0]\n",
      " [  0   0  24   0]\n",
      " [  0   0   0  92]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  45  test_size =  0.1\n",
      "0.979863190541\n",
      "[[169   0   0   0]\n",
      " [  1 134   2   1]\n",
      " [  0   0  23   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  45  test_size =  0.2\n",
      "0.97746931553\n",
      "[[169   0   0   0]\n",
      " [  1 133   1   1]\n",
      " [  0   0  24   0]\n",
      " [  0   1   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  45  test_size =  0.3\n",
      "0.977473522053\n",
      "[[168   1   0   0]\n",
      " [  2 133   0   1]\n",
      " [  0   0  25   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  45  test_size =  0.4\n",
      "0.985785975596\n",
      "[[169   1   0   0]\n",
      " [  1 133   0   1]\n",
      " [  0   0  25   0]\n",
      " [  0   0   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  50  test_size =  0.1\n",
      "0.986946794499\n",
      "[[169   1   0   0]\n",
      " [  1 133   0   0]\n",
      " [  0   0  24   0]\n",
      " [  0   0   1  92]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  50  test_size =  0.2\n",
      "0.97984179477\n",
      "[[169   0   0   0]\n",
      " [  1 133   2   0]\n",
      " [  0   0  23   0]\n",
      " [  0   1   0  92]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  50  test_size =  0.3\n",
      "0.983349672197\n",
      "[[169   0   0   0]\n",
      " [  1 133   1   1]\n",
      " [  0   0  24   0]\n",
      " [  0   1   0  91]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "=============no of estimators =  50  test_size =  0.4\n",
      "0.982210249064\n",
      "[[169   0   0   0]\n",
      " [  1 134   1   0]\n",
      " [  0   0  24   0]\n",
      " [  0   0   0  92]]\n",
      "['Austen', 'London', 'Shakespeare', 'Milton']\n",
      "[0.97163120567375882, 0.97508896797153022, 0.98201438848920863, 0.95744680851063835, 0.98220640569395012, 0.98561151079136688, 0.96099290780141844, 0.98932384341637014, 0.9928057553956835, 0.96808510638297873, 0.97508896797153022, 0.97841726618705038, 0.96453900709219853, 0.97864768683274017, 0.98920863309352514, 0.96099290780141844, 0.97508896797153022, 0.97122302158273377, 0.95744680851063835, 0.98576512455516019, 0.98920863309352514, 0.94326241134751776, 0.97508896797153022, 0.98920863309352514, 0.96453900709219853, 0.98932384341637014, 0.9928057553956835, 0.96099290780141844, 0.98576512455516019, 0.98920863309352514, 0.95390070921985815, 0.98576512455516019, 0.98561151079136688, 0.96808510638297873, 0.99288256227758009, 0.9928057553956835, 0.97163120567375882, 0.98220640569395012, 0.98561151079136688, 0.96808510638297873, 0.98932384341637014, 0.98201438848920863, 0.96808510638297873, 0.98576512455516019, 0.98920863309352514, 0.96453900709219853, 0.97508896797153022, 0.99640287769784175, 0.95744680851063835, 0.99288256227758009, 0.98920863309352514, 0.96453900709219853, 0.98932384341637014, 0.9928057553956835, 0.96099290780141844, 0.98576512455516019, 0.9928057553956835, 0.96453900709219853, 0.98932384341637014, 0.98920863309352514, 0.96453900709219853, 0.97864768683274017, 0.99640287769784175, 0.96099290780141844, 0.98220640569395012, 0.98920863309352514, 0.95744680851063835, 0.98576512455516019, 0.98920863309352514, 0.97163120567375882, 0.98932384341637014, 0.99640287769784175, 0.98226950354609932, 0.98576512455516019, 0.9928057553956835, 0.96099290780141844, 0.98932384341637014, 0.98920863309352514, 0.98226950354609932, 0.98576512455516019, 0.98201438848920863, 0.97517730496453903, 0.97864768683274017, 0.9928057553956835]\n"
     ]
    }
   ],
   "source": [
    "# create a test and training set\n",
    "all_scores=[]\n",
    "for nestimator in range(20,55,5):\n",
    "    for tsize in [x / 10.0 for x in range(1, 5)]:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(authorship[features], authorship.Author_num.values, test_size=0.5, random_state=123)\n",
    "        x, y = authorship[features], authorship.Author_num.values\n",
    "        # Fit Model\n",
    "        from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "        etclf = ExtraTreesClassifier(n_estimators=nestimator)\n",
    "        etclf.fit(x_train, y_train)\n",
    "\n",
    "        scores = cross_val_score(etclf, x, y)\n",
    "        all_scores.extend(scores)\n",
    "        print \"=============no of estimators = \", nestimator, \" test_size = \",tsize\n",
    "        print scores.mean()\n",
    "\n",
    "        # Print Confusion Matrix\n",
    "        print metrics.confusion_matrix(etclf.predict(x_test), y_test)\n",
    "        print authors\n",
    "\n",
    "print all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndf = pd.DataFrame(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.977811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.015540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.932624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.970669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.982206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.989209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.996403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "count  84.000000\n",
       "mean    0.977811\n",
       "std     0.015540\n",
       "min     0.932624\n",
       "25%     0.970669\n",
       "50%     0.982206\n",
       "75%     0.989209\n",
       "max     0.996403"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DecisionTreehas a score of', 1.0, ' in 0.015556s')\n",
      "('RandomForest with 30 estimatorshas a score of', 1.0, ' in 0.071715s')\n",
      "('ExtraTrees with 30 estimatorshas a score of', 1.0, ' in 0.048867s')\n",
      "('AdaBoost with 30 estimatorshas a score of', 1.0, ' in 0.21631s')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn import metrics\n",
    "from numpy import random\n",
    "\n",
    "# Matplotlib Styles\n",
    "mplc = {'axes.labelsize': 17,\n",
    "'axes.titlesize': 16,\n",
    "'figure.figsize': [18, 8],\n",
    "'grid.linewidth': 1.6,\n",
    "'legend.fontsize': 17,\n",
    "'lines.linewidth': 2,\n",
    "'lines.markeredgewidth': 0.0,\n",
    "'lines.markersize': 11,\n",
    "'patch.linewidth': 0.5,\n",
    "'xtick.labelsize': 16,\n",
    "'xtick.major.pad': 20,\n",
    "'xtick.major.width': 2,\n",
    "'xtick.minor.width': 1,\n",
    "'ytick.labelsize': 16.0,\n",
    "'ytick.major.pad': 20,\n",
    "'ytick.major.width': 2,\n",
    "'ytick.minor.width': 1 }\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import clone\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Parameters\n",
    "n_classes = 69\n",
    "n_estimators = 30\n",
    "plot_colors = \"ryb\"\n",
    "cmap = plt.cm.RdYlBu\n",
    "plot_step = 0.02  # fine step width for decision surface contours\n",
    "plot_step_coarser = 0.5  # step widths for coarse classifier guesses\n",
    "RANDOM_SEED = 13  # fix the seed on each iteration\n",
    "\n",
    "# Load data\n",
    "#iris = load_iris()                      \n",
    "#\n",
    "\n",
    "plot_idx = 1\n",
    "\n",
    "models = [DecisionTreeClassifier(max_depth=None),\n",
    "          RandomForestClassifier(n_estimators=n_estimators),\n",
    "          ExtraTreesClassifier(n_estimators=n_estimators),\n",
    "          AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "                             n_estimators=n_estimators)]\n",
    "\n",
    "authorship = read_csv('../data/authorship.csv')\n",
    "authors = list(set(authorship.Author.values))\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(authors)\n",
    "authorship['Author_num'] = le.transform(authorship['Author'])\n",
    "#authorship.drop(['BookID','Author'],1,inplace=True)\n",
    "features = list(authorship.columns)\n",
    "features.remove('Author')\n",
    "features.remove('Author_num')\n",
    "features.remove('BookID')\n",
    "\n",
    "import time\n",
    "\n",
    "with sns.plotting_context(\"talk\", font_scale=1, rc=mplc):\n",
    "\n",
    "\n",
    "#    for pair in ([0, 1], [0, 2], [2, 3]):\n",
    "        for model in models:\n",
    "            # We only take the two corresponding features\n",
    "#            X = iris.data[:, pair]\n",
    "#            y = iris.target\n",
    "\n",
    "            X = np.array(authorship[features])\n",
    "            y = np.array(authorship.Author_num.values)\n",
    "\n",
    "#            print type(X), X.shape\n",
    "#            print type(y), y.shape\n",
    "\n",
    "            # Shuffle\n",
    "            idx = np.arange(X.shape[0])\n",
    "            np.random.seed(RANDOM_SEED)\n",
    "            np.random.shuffle(idx)\n",
    "            X = X[idx]\n",
    "            y = y[idx]\n",
    "\n",
    "#            print type(X), X.shape\n",
    "#            print type(y), y.shape\n",
    "            \n",
    "            # Standardize\n",
    "            mean = X.mean(axis=0)\n",
    "            std = X.std(axis=0)\n",
    "            X = (X - mean) / std\n",
    "\n",
    "            # Train\n",
    "            clf = clone(model)\n",
    "            start = time.clock()\n",
    "            clf = model.fit(X, y)\n",
    "            end = time.clock()\n",
    "\n",
    "            scores = clf.score(X, y)\n",
    "            # Create a title for each column and the console by using str() and\n",
    "            # slicing away useless parts of the string\n",
    "            model_title = str(type(model)).split(\".\")[-1][:-2][:-len(\"Classifier\")]\n",
    "            model_details = model_title\n",
    "            if hasattr(model, \"estimators_\"):\n",
    "                model_details += \" with {} estimators\".format(len(model.estimators_))\n",
    "            print( model_details + \"has a score of\", scores, \" in \" + str(end-start) + \"s\"  )\n",
    "\n",
    "#\n",
    "# CAN'T FIGURE OUT THE PLOTTING\n",
    "#\n",
    "\n",
    "#            plt.subplot(3, 4, plot_idx)\n",
    "#            if plot_idx <= len(models):\n",
    "                # Add a title at the top of each column\n",
    "#                plt.title(model_title)\n",
    "\n",
    "            # Now plot the decision boundary using a fine mesh as input to a\n",
    "            # filled contour plot\n",
    "#            x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "#            y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "#            xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "#                                 np.arange(y_min, y_max, plot_step))\n",
    "#            print \"x_min = \", x_min\n",
    "#            print \"x_max = \", x_max\n",
    "#            print \"y_min = \", y_min\n",
    "#            print \"y_max = \", y_max\n",
    "#            print \"xx = \", xx\n",
    "#            print \"yy = \", yy\n",
    "#            print \"len(xx.ravel) = \", len(xx.ravel())\n",
    "#            print \"len(yy.ravel) = \", len(yy.ravel())\n",
    "#            print \"len(np.c_[xx.ravel(), yy.ravel()]) = \", len(np.c_[xx.ravel(), yy.ravel()])\n",
    "            \n",
    "            # Plot either a single DecisionTreeClassifier or alpha blend the\n",
    "            # decision surfaces of the ensemble of classifiers\n",
    "#            if isinstance(model, DecisionTreeClassifier):\n",
    "#                Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "#                Z = Z.reshape(xx.shape)\n",
    "#                cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "#            else:\n",
    "                # Choose alpha blend level with respect to the number of estimators\n",
    "                # that are in use (noting that AdaBoost can use fewer estimators\n",
    "                # than its maximum if it achieves a good enough fit early on)\n",
    "#                estimator_alpha = 1.0 / len(model.estimators_)\n",
    "#                for tree in model.estimators_:\n",
    "#                    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "#                    Z = Z.reshape(xx.shape)\n",
    "#                    cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n",
    "\n",
    "            # Build a coarser grid to plot a set of ensemble classifications\n",
    "            # to show how these are different to what we see in the decision\n",
    "            # surfaces. These points are regularly space and do not have a black outline\n",
    "#            xx_coarser, yy_coarser = np.meshgrid(np.arange(x_min, x_max, plot_step_coarser),\n",
    "#                                                 np.arange(y_min, y_max, plot_step_coarser))\n",
    "#            Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(), yy_coarser.ravel()]).reshape(xx_coarser.shape)\n",
    "#            cs_points = plt.scatter(xx_coarser, yy_coarser, s=15, c=Z_points_coarser, cmap=cmap, edgecolors=\"none\")\n",
    "\n",
    "            # Plot the training points, these are clustered together and have a\n",
    "            # black outline\n",
    "##            for i, c in zip(xrange(n_classes), plot_colors):\n",
    "#                idx = np.where(y == i)\n",
    "#                plt.scatter(X[idx, 0], X[idx, 1], c=c, label=iris.target_names[i],\n",
    "#                            cmap=cmap)\n",
    "\n",
    "#            plot_idx += 1  # move on to the next plot in sequence\n",
    "\n",
    "#        plt.suptitle(\"Classifiers on feature subsets of the Iris dataset\")\n",
    "#        plt.axis(\"tight\")\n",
    "\n",
    "#        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
